---
title: "Check your outliers! An accessible introduction to identifying statistical outliers in R with *easystats*"

authors:
- affiliation: 1
  name: Rémi Thériault
  orcid: 0000-0003-4315-6788
- affiliation: 2
  name: Mattan S. Ben-Shachar
  orcid: 0000-0002-4287-4801
- affiliation: 3
  name: Indrajeet Patil
  orcid: 0000-0003-1995-6531
  
affiliations:
- index: 1
  name: Université du Québec à Montréal, Montréal, Québec
- index: 2
  name: XXX
- index: 3
  name: Center for Humans and Machines, Max Planck Institute for Human Development, Berlin, Germany 
  
output: 
  bookdown::pdf_document2:
    toc: false
    keep_tex: true
    latex_engine: xelatex

bibliography: paper.bib
csl: apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  comment = "#>",
  out.width = "100%",
  dpi = 300,
  warning = FALSE
)

library(performance)
library(see)
library(datawizard)
```

<!-- IP: Should there be equations? The title says "accessible" introduction to outlier detection methods, but if we don't do a good job of explaining these equations, we might be compromise on the accessibility. -->

# Abstract

xyz

# Introduction

<!-- What is an outlier -->

Real-life data often contain observations that can be considered as *abnormal* when compared to the main population. The cause of it - be it because they belong to a different distribution (originating from a different generative process) or simply being extreme cases, statistically rare but not impossible - can be hard to access, and the boundaries of "abnormal" hard to define. 

<!-- The problem with outliers -->

Nonetheless, the improper handling of these outliers can substantially affect statistical model estimations,
<!-- thus contributing to false positives [@simmons2011false] as well as false negatives. [[Dom: False positive/negatives of what? Are we talking about a classification case? I'd replace with:]] -->
biasing effect estimation and weakening the models' predictive performance. 
It is thus essential to address this problem in a thoughtful manner. Despite the existence of established recommendations and guidelines, many researchers still do not treat outliers in a consistent manner, or do so using inappropriate strategies [@simmons2011false; @leys2013outliers].
<!-- Fortunately, guidelines exist in this regard. Yet, especially in the field of psychology [[Dom: that would probably need a ref lol]], many researchers still do not treat outliers in a consistent manner or do so using inappropriate strategies [@simmons2011false; @leys2013outliers]. -->

<!-- What this paper is about -->

One possible reason is that researchers are not aware of the existing recommendations, or do not know how to implement them using their analysis software. In this paper, we show how to follow current best practices for automatic and reproducible statistical outlier detection (SOD) using R and the *{performance}* package [@ludecke2021performance] from the *easystats* ecosystem <!-- Dom: add Ref here of easystats -->.

# Identifying Outliers

<!-- upfront with the subjectivity and limitations of automatic SOD blabla -->
Which exact outlier method to use depends on many factors. In some cases, eye-gauging odd observations can be an appropriate solution, though many researchers will favour algorithmic solutions to detect potential outliers, i.e., based on a mathematical score. Importantly, whatever approach is chosen remains a subjective decision of the researcher which usage (and rationale) must be transparently documented and reproducible [@leys2019outliers]. Researchers should commit (ideally in a preregistration) to an outlier treatment method before collecting the data. They should report in the paper their decisions and details of their methods, as well as any deviation from their original plan. These transparency practices can help reduce false positives due to excessive researchers' degrees of freedom (choice flexibility throughout the analysis).

<!-- Overview of sections -->
One of the factor to consider when selecting a outlier detection method is the statistical test of interest. When using a regression model, for example, relevant information can be found by identifying observations that do not fit well with the model. This approach, known as model-based outliers detection (as outliers are extracted after the statistical model has been fit), can be opposed to distribution-based outliers detection, which is based on the distance between an observation and the "center" of its population. There, various quantifications of distance exist, some being univariate (involving only one variable) or multivariate. In the following section, we will go through each of the mentioned methods and provide examples on how to implement them with R.

## Univariate Outliers

One common method used by researchers is to identify outliers with measures of deviation from the center of a variable's distribution. This approach can be appropriate when the goal is to analyze a subset of independent variables using simple tests (e.g., *t*-tests or correlations). One of the most popular procedure is the z-scores transformation, which computes the distance in standard deviation (SD) from the mean. Cut-offs are often used, such as "+/- 2 SDs" (more exactly, 1.96), which will yield the ~2.5\% most extreme values on both sides - assuming a normally distributed variable. 

<!-- MSB: t-tests and correlations are model/multivariable statistics, so univariate outlier methods might give false-positives... RT: Sure, so when would you use them? Only when you have no other variables? How would you convey this idea in writting for the paper? What do you think of what I wrote above? And do you have any reference for that? -->

<!-- Slowly introduce chec_outlier
-->
Researchers can identify outliers based on their z-score using the `check_outliers()` in the  *{performance}* package. Although @leys2013outliers suggest a threshold of +/-2.5 and @leys2019outliers a threshold of +/-3, *{performance}* uses by default a less conservative default threshold of ~3.09t.^[ 3.09 is an approximation of the critical value for _p_ < .001, obtained through `qnorm(.999)`. We chose this threshold for consistency with the thresholds of all our other methods.] That is, data points will be flagged as outliers if they go beyond +/- ~3.09 SD. However, users can adjust this threshold using the `threshold` argument, as demonstrated below:


```{r z_score}
library(performance)

# Create a variable with outliers
var <- c(1, 2, 3, 2, 1, 666, 1, 2, 3, 2, 1)

# Use default threshold to identify outliers
check_outliers(var, method = "zscore")
```
```{r z_score_threshold}
# Change threshold
check_outliers(var, method = "zscore", threshold = 2.5)
```
<!-- Dom: Note: I wouldn't showcase the ID arg as it would be nice to replace it by `select` in the future to be consistent with the other functions -->
<!-- Dom (edit): sorry I got confused didnt get that ID corresponded to the rownames... disregard the comment above ^^ -->

However, this approach presents some important limitations, as the mean and standard deviation themselves are not robust to the influence of outliers and they assume a symmetric and Gaussian distribution. One easy alternative is to compute the distribution center and deviation using methods more robust to extreme values, such as the median and the Median Absolute Deviation (MAD) [@leys2019outliers; @leys2013outliers; @leys2018outliers], though this approach shares many of the limitations of the former. One can use this method in R by changing the method to `method = "zscore_robust"`, store its results, and re-use it to remove the corresponding observations from the dataset.


<!-- Dom: Is the MAD definition necessary? I'd skip --> 
<!-- $$ -->
<!-- MAD = b M_i(|x_i-M_j(x_j)|) -->
<!-- $$ -->
<!-- Where $b$ is a scaler, often set to $1/\left(\Phi ^{-1}(3/4)\right)\approx 1.4826$. -->



```{r univariate outliers}
# Load toy dataset available in R
data <- mtcars 

# Replace 6th and 10th observation by odd values
data[6, 1:5] <- rep(-5, 5)
data[10, 1:5] <- rep(50, 5)

outliers <- check_outliers(data$mpg, method = "zscore_robust")
outliers
```



The row names of the detected outliers can be obtained by using `which()` on the output object, which can be used for exclusions for example:

```{r}
which(outliers)

data_clean <- data[-which(outliers), ]
```

All `check_outliers()` output objects possess a `plot()` method, meaning it is also possible to visualize the outliers:

(ref:univariate) Visual depiction of outliers using the robust _z_-score method.

```{r univariate, fig.cap = '(ref:univariate)'}
library(see)

plot(outliers)
```

Other univariate methods are available, such as using the interquartile range (IQR), or based on different intervals, such as the Highest Density Interval (HDI) or the Bias Corrected and Accelerated Interval (BCI). These methods are documented and described in the function's [help page](https://easystats.github.io/performance/reference/check_outliers.html).

## Multivariate Outliers

Univariate outliers can be useful when the focus is on a particular variabe, for instance the reaction time, as extreme values might be indicative of inattention or non-task-related behavior^[ Note that they might not be the optimal way of treating reaction time outliers, see Ratcliff (1993) and Van Zandt et al. (1995).].
<!-- Van Zandt, T., & Ratcliff, R. (1995). Statistical mimicking of reaction time data: Single-process models, parameter variability, and mixtures. Psychonomic Bulletin & Review, 2(1), 20-54.-->
<!-- Ratcliff, R. (1993). Methods for dealing with reaction time outliers. Psychological bulletin, 114(3), 510. -->
However, in many scenarios, variables of a dataset are not independent, and an abnormal observation will impact multiple dimensions. Let's imagine a participant giving random answers to a questionnaire. In this case, computing the z-score for each of the questions might not lead to satisfactory results. Instead, one might want to look at these variables together. One popular approach is to compute multivariate distance metrics such as the Mahalanobis distance. However, another possibility is to use its robust alternative, namely the Minimum Covariance Determinant [MCD, @leys2018outliers; @leys2019outliers]. The MCD can be calculated as follow [@hubert2018mcd]:

$$
MATTHS_{gohere}^{andB}
$$

**[mattansb can you add some maths here; perhaps from @hubert2018mcd? https://doi.org/10.1002/wics.1421]**

In {performance}'s `check_outliers()`, one can use this approach with `method = "mcd"`.^[Our default threshold for the MCD method is defined by `stats::qchisq(p = 1 - 0.001, df = ncol(x))`, which again is an approximation of the critical value for _p_ < .001 consistent with the thresholds of our other methods.]=

(ref:multivariate) Visual depiction of outliers using the Minimum Covariance Determinant (MCD) method, a robust version of the Mahalanobis distance.

```{r multivariate, fig.cap = '(ref:multivariate)'}
results <- check_outliers(data, method = "mcd")
results

plot(results)
```

## Model-Based Outliers

Working with regression models open the possibility of using model-based SOD methods. These methods rely on the concept of *leverage*, i.e., how much influence a given observation can have on the whole model. If few observations have a relatively strong leverage/influence on the model, one can suspect that the model's estimates are biased by these observations, in which case flagging them as outliers could prove helpful. The Cook's distance, an example of such a model-based SOD, can be calculated as follow:

$$
MATTHS_{gohere}^{andB}
$$

**[mattansb can you add some maths here?]**

<!-- MSB: Honesly, I would just copy from wikipedia... https://en.wikipedia.org/wiki/Cook%27s_distance -->

The `check_outliers()` cam ne applied directly on regression models, specifying `method = "cook"` (or `method = "pareto"` for Bayesian models).^[Our default threshold for the Cook method is defined by `stats::qf(0.5, ncol(x), nrow(x) - ncol(x))`, which again is an approximation of the critical value for _p_ < .001 consistent with the thresholds of our other methods.]

(ref:model) Visual depiction of outliers based on Cook's distance (leverage and standardized residuals).

<!-- RT: Is this what this plot is called?? Is it using Cook's distance or what? -->

```{r model, fig.cap = '(ref:model)'}
model <- lm(disp ~ mpg * hp, data = data)
x <- check_outliers(model, method = "cook")
x

plot(x)
```

Interestingly, @leys2018outliers report a preference for the MCD method over Cook's distance. This is because Cook's distance removes one observation at a time and checks its corresponding influence on the model each time [@cook1977detection], and flags any observation that has a large influence. In the view of these authors, when there are several outliers, the process of removing a single outlier at a time is problematic as the model remains "contaminated" or influenced by other possible outliers in the model, rendering this method suboptimal in the presence of multiple outliers.

That said, distribution-based approaches are not a silver bullet either, and there are cases where the usage of methods agnostic to theoretical and statistical models of interest might be problematic. For example, a very tall person would be expected to also be much heavier than average, but that would still fit with the expected relationship between height and weight (i.e., would be in line with a model such as `age ~ weight`). In contrast, using multivariate outlier detection methods there may flag this person as being an outlier - being unusual on two variables, height and weight = even though the pattern fits perfectly with our predictions.

Furthermore, unusual observations happen naturally: extreme observations are also expected even when taken from a normal distribution. While statistical models can integrate this "expectation", multivariate outlier methods might be too conservative, flagging too many observations despite belonging to the right generative process. Finally, certain classes of models are directly more robust to outliers, like _t_ regression or quantile regression, rendering their precise identification less critical.

## Multiple Methods

An alternative approach suggested by _easystats_ is to combine several methods, based on the assumption that different methods provide different angles of looking at the problem. By applying a variety of methods, one can hope to "triangulate" the true outliers (those consistently flagged by multiple methods) and minimize false positive. In practice, this approach computes a composite outlier score, formed of the average of the binary (0 or 1) classification results of each method. It represents the probability that each observation is classified as an outlier by at least one method. The default decision rule classifies rows with composite outlier scores superior or equal to 0.5 as outlier observations (i.e., that were classified as outliers by at least half of the methods). In {performance}'s `check_outliers()`, one can use this approach by including all desired methods in the corresponding argument.

(ref:multimethod) Visual depiction of outliers using several different statistical outlier detection methods.

```{r multimethod, fig.cap = '(ref:multimethod)'}
results <- check_outliers(data[1:5], method = c("zscore", "iqr", "mcd", "ics"))
results

plot(results)
```

Outliers (counts or per variables) for individual methods can then be obtained through attributes. For example:

```{r}
attributes(results)$outlier_var$iqr
```

An example sentence for reporting the usage of the composite method could be:

> Based on a composite outlier score [see the 'check_outliers' function in the 'performance' R package, @ludecke2021performance] obtained via the joint application of multiple outliers detection algorithms [(a) median absolute deviation (MAD)-based robust z-scores, @leys2013outliers; (b) interquartile range (IQR), (c) Mahalanobis minimum covariance determinant (MCD), @leys2019outliers; and (d) invariant coordinate selection (ICS), @archimbaud2018ics], we excluded three participants that were classified as outliers by at least half of the methods used.

<!-- Dom: note: we should add support to this in report to create this sentence... -->

# Handling Outliers

The above section demonstrated how to identify outliers using the `check_outliers()` function in the *{performance}* package. But what should we do with these outliers once identified? Although it is common to automatically discard any observation that has been marked as "an outlier" as if it might infect the rest of the data with its statistical ailment, we believe that the use of SOD methods is but one step in the get-to-know-your-data pipeline; a researcher or analyst's _domain knowledge_ must be involved in the decision of how to deal with observations marked as outliers by means of SOD. Indeed, automatic tools can help detect outliers, but they are nowhere near perfect. Although they can be useful to flag suspect data, they can have misses and false alarms, and they cannot replace human eyes and proper vigilance from the researcher.

## Error, Interesting, and Random Outliers

@leys2019outliers distinguish between error outliers, interesting outliers, and random outliers.

_Error outliers_ are likely due to human error and should be corrected before data analysis or outright removed since they are invalid observations. _Interesting outliers_ are not due to technical error and may be of theoretical interest; it might thus be relevant to investigate them further even though they should be removed from the current analysis of interest. _Random outliers_ are assumed to be due to chance alone and to belong to the correct distribution and, therefore, should be retained.

It is recommended to _keep_ observations which are expected to be part of the distribution of interest, even if they are outliers [@leys2019outliers]. However, if it is suspected that the outliers belong to an alternative distribution, then those observations could have a large impact on the results and call into question their robustness, especially if significance is conditional on their inclusion.

On the other hand, there are also outliers that cannot be detected by statistical tools, but should be found and removed. For example, if we're studying the effects of X on Y among teenagers and we have one observation from a 20-year-old, this observation might not be a _statistical outlier_, but it is an outlier in the _context_ of our research, and should be discarded to allow for valid inferences of interest.

_Removing_ outliers can in this case be a valid strategy, and ideally one would report results with and without outliers to see the extent of their impact on results. This approach however can reduce statistical power. Therefore, some propose a _recoding_ approach, namely, winsorization: bringing outliers back within acceptable limits [e.g., 3 MAD. @tukey1963less]. However, if possible, it is recommended to collect enough data so that even after removing outliers, there is still sufficient statistical power without having to resort to winsorization [@leys2019outliers].

## Winsorization

Above, we mentioned a recoding approach to handling outliers: winsorization [@tukey1963less]. The _easystats_ ecosystem makes it easy to incorporate this step into your workflow through the `winsorize()` function of the {datawizard} package [@patil2022datawizard]. This procedure will bring back univariate outliers within the limits of 'acceptable' values, based either on the percentile, the _z_ score, or, ideally, the robust _z_ score (based on the MAD).

Example:

```{r winsorization}
data[33:34, ]

# winsorizing using the MAD
library(datawizard)
winsorized.data <- winsorize(data, method = "zscore", robust = TRUE, threshold = 3)

winsorized.data[33:34, ]
```

## The Importance of Transparency

We note that no matter which outlier method you use, the handling of outliers should be specified *a priori* with as much detail as possible, and ideally preregistered, to limit researchers' degrees of freedom and therefore risks of false positives [@leys2019outliers]. This is especially true given that interesting outliers and random outliers are oftentimes hard to distinguish in practice. Thus, researchers should always prioritize transparency and report all of the following information: (a) how many outliers were identified; (b) according to which method and criteria, (c) using which function which R package (if applicable), and (d) how they were handled (excluded or winsorized, if the latter, using what threshold). If at all possible, (e) the corresponding code script along with the data should be shared on a public repository like the Open Science Framework, so that the exclusion criteria can be reproduced precisely.

# Conclusion

<!-- IP: Once the rest of the content is finalized, we need to give this another shot. The current version doesn't jibe with me. -->

<!-- MSB: Same - I think we should address the need for human eyes on data. Automatic tools are good for flagging suspect data, but they can have misses and false alarms. RT: What about the extra sentence I added in the "handling outliers" section? -->

In this paper, we have showed how to investigate outliers using the `check_outliers()` function of the {performance} package while following current good practices. We note that in addition to using the current functions and respecting existing recommendations, it is also important to pre-specify your plans to manage outliers, such as with preregistration, or at the very least justify your choices and stay consistent. Ideally, one would additionally also report the package, function, and threshold used (ideally linking to the full code). We hope that this paper will help more researchers engage in good research practices while providing a smooth outlier detection experience.

# Acknowledgments

{performance} is part of the collaborative [*easystats*](https://github.com/easystats/easystats) ecosystem. Thus, we thank all [members of easystats](https://github.com/orgs/easystats/people), contributors, and users alike.

# References
