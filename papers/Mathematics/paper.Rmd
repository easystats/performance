---
title: "Check your outliers! An accessible introduction to identifying statistical outliers in R with *easystats*"
subtitle: "(Alt title:) Detecting Statistical Outliers: Univariate, Multivariate, and Model-Based"

authors:
- affiliation: 1
  name: Rémi Thériault
  orcid: 0000-0003-4315-6788
- affiliation: 2
  name: Mattan S. Ben-Shachar
  orcid: 0000-0002-4287-4801

affiliations:
- index: 1
  name: Université du Québec à Montréal, Montréal, Québec
- index: 2
  name: XXX

output: 
  pdf_document:
    keep_tex: true
    latex_engine: xelatex

bibliography: paper.bib
csl: apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  comment = "#>",
  out.width = "100%",
  dpi = 300,
  warning = FALSE
)

library(performance)
library(see)
library(datawizard)
```

<!-- IP: Should there be equations? The title says "accessible" introduction to outlier detection methods, but if we don't do a good job of explaining these equations, we might be compromise on the accessibility. -->

<!-- IP: All figures need to have captions. -->

# Abstract

xyz

# Introduction

The improper handling of outliers can substantially affect statistical model estimations, thus contributing to false positives [@simmons2011false] but almost certainly to false negatives as well. It is thus essential to address this problem in a thoughtful manner. Fortunately, guidelines exist in this regard. Yet, especially in the field of psychology, many researchers still do not treat outliers in a consistent manner or do so using inappropriate strategies [@simmons2011false; @leys2013outliers].

One possible reason is that researchers do not know of existing recommendations or currently available software options for their implementation. In this paper, we show how to follow current recommendations for statistical outlier detection (SOD) using R and {performance} package [@ludecke2021performance] from *easystats* ecosystem.

<!-- Use of brackets or italics for packages seem inconsistent among our papers. What should we use here? -->
<!-- IP: We now tend to use bracket notation {} for package names, so we should stick to that convention henceforth. But when we want to refer to the entire ecosystem (and not the package), we can use italics -->

<!-- Add a "performance is part of the easystats ecosystem" paragraph? -->

<!-- Add a "Compared to other packages" paragraph? -->
<!-- I have changed the title to specify that we are specifically focusing on easystats, which means we don't need to get into the discussion about the available outliers. -->

# Identifying Outliers

Although many researchers attempt to identify outliers with measures based on the mean (e.g. z-scores), those methods are problematic because the mean and standard deviation themselves are not robust to the influence of outliers and they assume a normal distribution [@leys2019outliers; @leys2013outliers]. Therefore, current guidelines [ref?] recommend using robust methods to identify outliers, such as those relying on the median as opposed to the mean.

Nonetheless, which exact outlier method to use depends on several factors, like the statistical test of interest. When using a regression model, for example, the most relevant information will be regarding observations that do not fit well with the model, known as model-based outliers. When no method is readily available to detect model-based outliers, such as for structural equation modelling (SEM), looking for multivariate outliers maybe relevant. Finally, for simple tests (t-tests or correlations) that compare values of the same variable, it makes sense to check for univariate outliers. In the following section, we will go through each of the mentioned methods.

<!-- MSB: t-tests and correlations are model/multivariable statistics, so univariate outlier methods might give false-positives... -->

<!-- Any references for this or is it just common sense? -->

## Univariate Outliers

For univariate outliers, it is recommended to use the median along the Median Absolute Deviation (MAD), which is more robust than the interquartile range or the mean and its standard deviation [@leys2019outliers; @leys2013outliers]. The MAD can be calculated as follow:

$$
MAD = b M_i(|x_i-M_j(x_j)|)
$$
Where $b$ is a scaler, often set to $1/\left(\Phi ^{-1}(3/4)\right)\approx 1.4826$.

In {performance}'s `check_outliers()`, one can use this approach with `method = "zscore_robust"`. Although @leys2013outliers suggests a default threshold of 2.5 and @leys2019outliers a threshold of 3, for consistency with other outlier detection methods, {performance} uses a less conservative threshold of 3.09 by default. That is, data points will be flagged as outliers if they go beyond +/- 3.09 MAD. Users can adjust this threshold using `threshold` argument.

<!-- Question: should we change our default threshold to align with recommendations?? -->

Example:

```{r univariate outliers}
library(performance)

# create some fake outliers and an ID column
data <- rbind(mtcars[1:4], 42, 55)
data <- cbind(car = row.names(data), data)

x <- check_outliers(data, method = "zscore_robust", ID = "car")
x
```

All `check_outliers()` output objects possess a `plot()` method, meaning it is also possible to visualize the outliers:

```{r univariate plot}
library(see)
plot(x)
```

## Multivariate Outliers

For multivariate outliers, it is recommended to use the Minimum Covariance Determinant, a robust version of the Mahalanobis distance [MCD, @leys2019outliers]. In {performance}'s `check_outliers()`, one can use this approach with `method = "mcd"`. The MCD can be calculated as follow [@hubert2018mcd]:

$$
MATTHS_{gohere}^{andB}
$$

**[@mattansb can you add some maths here; perhaps from @hubert2018mcd? https://doi.org/10.1002/wics.1421]**

<!-- @leys2019outliers write, "MCD75 (breakdown point = 0.25) (or the MCD50 (breakdown point = 0.5) if you suspect the presence of more than 25% of outlying values)". We use a threshold of `stats::qchisq(p = 1 - 0.001, df = ncol(x))` (18.467 in the example below). @bwiernik, how do these thresholds compare, and should we align our threshold with this recommendation? -->

Example:

```{r multivariate outliers}
x <- check_outliers(data, method = "mcd")
x

plot(x)
```

## Model-Based Outliers

[FIND REFERENCES TO SUPPORT MODEL-BASED OUTLIER DETECTION ABOVE ALL OTHER METHODS!!]

Something something... what is leverage... why we should care if few observations have (relatively) strong leverage (answer: they are suspect of biasing our estimates!).

<!-- @bwiernik I know you prefer model-based outlier detection methods over multivariate methods, and I do as well. Do we have any references to back this or is it just common sense?-->

When working with regression models, model-based outliers can be detected using `check_outliers()` by specifying `method = "cook"` (or `method = "pareto"` for Bayesian models). The Cook's distance can be calculated as follow:

$$
MATTHS_{gohere}^{andB}
$$

**[@mattansb can you add some maths here?]**

<!-- MSB: Honesly, I would just copy from wikipedia... https://en.wikipedia.org/wiki/Cook%27s_distance -->

Example:

```{r model-based outliers}
model <- lm(disp ~ mpg * hp, data = data)
x <- check_outliers(model, method = "cook")
x

plot(x)
```

## Multiple methods

An alternative approach suggested by easystats is to combine several methods. This approach computes a composite outlier score, formed of the average of the binary (0 or 1) results of each method. It represents the probability that each observation is classified as an outlier by at least one method. The default decision rule classifies rows with composite outlier scores superior or equal to 0.5 as outlier observations (i.e., that were classified as outliers by at least half of the methods). In {performance}'s `check_outliers()`, one can use this approach by including all desired methods in the corresponding argument.

Example:

```{r multimethod}
x <- check_outliers(
  data,
  method = c("zscore_robust", "iqr", "mcd", "ics"),
  ID = "car"
)
x

plot(x)
```

An example sentence for reporting the usage of the composite method could be:

> Based on a composite outlier score [see the 'check_outliers' function in the 'performance' R package, @ludecke2021performance] obtained via the joint application of multiple outliers detection algorithms [(a) median absolute deviation (MAD)-based robust z-scores, @leys2013outliers; (b) interquartile range (IQR), (c) Mahalanobis minimum covariance determinant (MCD), @leys2019outliers; and (d) invariant coordinate selection (ICS), @archimbaud2018ics], we excluded three participants that were classified as outliers by at least half of the methods used.

# Handling Outliers

We have at this point demonstrated how to identify outliers. But what should we do with these outliers once identified? Although it is common to automatically discard any observation that has been marked as "an outlier" as if it might infect the rest of the data with its statistical ailment, we believe that the use of SOD methods is but one step in the get-to-know-your-data pipeline; a researcher or analyst's _domain knowledge_ must be involved in the decision of how to deal with observations marked as outliers by means of SOD.<!-- MSB: I changed this paragraph as this is what I believe and teach. --> For example, @leys2019outliers distinguishes between error outliers, interesting outliers, and random outliers.

_Error outliers_ are likely due to human error and should be corrected before data analysis or outright removed since they are invalid observations. _Interesting outliers_ are not due to technical error and may be of theoretical interest; it might thus be relevant to investigate them further even though they should be removed from the current analysis of interest. _Random outliers_ are assumed to be due to chance alone and to belong to the correct distribution and, therefore, should be retained.

It is recommended to _keep_ observations which are expected to be part of the distribution of interest, even if they are outliers [@leys2019outliers]. However, if it is suspected that the outliers belong to an alternative distribution, then those observations could have a large impact on the results and call into question their robustness, especially if significance is conditional on their inclusion.

On the other hand, there are also outliers that cannot be detected by statistical tools, but should be found and removed. For example, if we're studying the effects of X on Y among teenages and we have one observation from a 20-year-old, this observation might not be a _statistical outlier_, but it is an outlier in the _context_ of our research, and should be discarded to allow for valid inferences of interest.

_Removing_ outliers can in this case be a valid strategy, and ideally one would report results with and without outliers to see the extent of their impact on results (assuming the study was not preregistered with a prespecified outlier treatment plan)<!-- MSB: arguably, prereg should not come in the way of this investigation... -->. This approach however can reduce statistical power. Therefore, some propose a _recoding_ approach; i.e., winsorizing [@tukey1963less] outliers to bring them back within acceptable limits (e.g., 3 MAD). However, if possible, it is recommended to collect enough data so that even after removing outliers, there is still sufficient statistical power without having to resort to winsorization [@leys2019outliers].

We note that no matter which outlier method you use, the handling of outliers should be specified *a priori* with as much detail as possible, and ideally preregistered, to limit researchers' degrees of freedom and therefore risks of false positives [@leys2019outliers]. This is especially true given that interesting outliers and random outliers are oftentimes hard to distinguish in practice. Thus, researchers should always prioritize transparency and report exactly how many and which outliers were handled, and share code so that the exclusion criteria can be reproduced.

<!-- MSB: This ^ point can be added in the intro to the strength of using numerical outlier detection methods. -->

## Winsorization

(Should we add a section on winsorization since we talk about this in the intro/recommendations?) <!-- MSB: Yes -->

```{r winsorization}
data[33:34, ]

# winsorizing using the MAD
library(datawizard)
winsorized.data <- winsorize(data, method = "zscore", robust = TRUE, threshold = 3)

winsorized.data[33:34, ]
```

# Conclusion

<!-- IP: Once the rest of the content is finalized, we need to give this another shot. The current version doesn't jibe with me. -->

<!-- MSB: Same - I think we should address the need for human eyes on data. Automatic tools are good for flagging suspect data, but they can have misses and false alarms. -->

In this paper, we have showed how to investigate outliers using the `check_outliers()` function of the {performance} package while following current good practices. We note that in addition to using the current functions and respecting existing recommendations, it is also important to pre-specify your plans to manage outliers, such as with preregistration, or at the very least justify your choices and stay consistent. Ideally, one would additionally also report the package, function, and threshold used (ideally linking to the full code). We hope that this paper will help more researchers engage in good research practices while providing a smooth experience.

# Acknowledgments

{performance} is part of the collaborative [*easystats*](https://github.com/easystats/easystats) ecosystem. Thus, we thank the [members of easystats](https://github.com/orgs/easystats/people) as well as the users.

# References
